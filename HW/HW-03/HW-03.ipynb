{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "#import optuna #bayesian optimization\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "ACCTAGE     546\n",
      "PHONE      1075\n",
      "POS        1075\n",
      "POSAMT     1075\n",
      "INV        1075\n",
      "INVBAL     1075\n",
      "CC         1075\n",
      "CCBAL      1075\n",
      "CCPURC     1075\n",
      "INCOME     1537\n",
      "LORES      1537\n",
      "HMVAL      1537\n",
      "AGE        1702\n",
      "CRSCORE     195\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load in data\n",
    "df_train = pd.read_csv('insurance_t.csv')\n",
    "df_val = pd.read_csv('insurance_v.csv')\n",
    "\n",
    "# Find columns with missing data in train and validation datasets\n",
    "miss_t = [col for col in df_train.columns if df_train[col].isnull().sum() > 0]\n",
    "miss_v = [col for col in df_val.columns if df_val[col].isnull().sum() > 0]\n",
    "\n",
    "# Check if train and validation datasets are missing the same columns\n",
    "print(miss_t == miss_v)\n",
    "\n",
    "# Check what and how many values are missing in the train dataset\n",
    "missing_counts_train = df_train[miss_t].isnull().sum()\n",
    "print(missing_counts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns w/ missingness\n",
    "continuous_cols = [\"ACCTAGE\", \"PHONE\", \"POS\", \"POSAMT\", \"INVBAL\", \"CCBAL\", \"INCOME\", \"LORES\", \"HMVAL\", \"AGE\", \"CRSCORE\"]\n",
    "binary_cols = [\"INV\", \"CC\", \"CCPURC\"]\n",
    "\n",
    "#replace ns's with 'Missing', 1's w/ 'Instance', 0' w/ 'Non-Instance' for train and val\n",
    "for col in binary_cols:\n",
    "    df_train[col] = df_train[col].replace({1: 'Instance', 0: 'Non-Instance'}).fillna('Missing')\n",
    "    df_val[col] = df_val[col].replace({1: 'Instance', 0: 'Non-Instance'}).fillna('Missing')\n",
    "\n",
    "\n",
    "#impute with median and create binary flag column\n",
    "for col in continuous_cols:\n",
    "    median_value = df_train[col].median()\n",
    "    # Create a binary flag variable for imputation\n",
    "    df_train[f'{col}_imputed'] = df_train[col].isnull().astype(int)\n",
    "    #impute the missing values with the median\n",
    "    df_train[col] = df_train[col].fillna(median_value)\n",
    "\n",
    "#do the same for val\n",
    "for col in continuous_cols:\n",
    "    median_value = df_train[col].median()\n",
    "    # Create a binary flag variable for imputation\n",
    "    df_val[f'{col}_imputed'] = df_val[col].isnull().astype(int)\n",
    "    #impute the missing values with the median\n",
    "    df_val[col] = df_val[col].fillna(median_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate binary columns and categorical columns for one-hot encoding\n",
    "binary_columns = [\n",
    "    \"INAREA\", \"SDB\", \"MM\", \"IRA\", \"CD\", \"ATM\", \"SAV\", \"NSF\", \"DIRDEP\", \"DDA\", \"ACCTAGE_imputed\", \"PHONE_imputed\", \"POS_imputed\", \n",
    "    \"POSAMT_imputed\", \"INVBAL_imputed\", \"CCBAL_imputed\", \n",
    "    \"INCOME_imputed\", \"LORES_imputed\", \"HMVAL_imputed\", \n",
    "    \"AGE_imputed\", \"CRSCORE_imputed\"\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    \"INV\", \"CC\", \"CCPURC\", \"BRANCH\"\n",
    "]\n",
    "\n",
    "# Apply one-hot encoding only to the specified categorical columns\n",
    "df_train = pd.get_dummies(df_train, columns=categorical_columns, drop_first=False)\n",
    "df_val = pd.get_dummies(df_val, columns=categorical_columns, drop_first=False)\n",
    "\n",
    "# Label encode the MMCRED column in both datasets\n",
    "label_encoder = LabelEncoder()\n",
    "df_train['MMCRED'] = label_encoder.fit_transform(df_train['MMCRED'])\n",
    "df_val['MMCRED'] = label_encoder.transform(df_val['MMCRED'])\n",
    "\n",
    "# Ensure both train and validation have the same columns\n",
    "df_val = df_val.reindex(columns=df_train.columns, fill_value=0)\n",
    "\n",
    "# Convert binary columns to 0/1 (if needed)\n",
    "for col in binary_columns:\n",
    "    df_train[col] = df_train[col].astype(int)\n",
    "    df_val[col] = df_val[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X as all columns except 'INS' and y as the 'INS' column\n",
    "X_train = df_train.drop(columns=['INS'])\n",
    "y_train = df_train['INS']\n",
    "\n",
    "X_val = df_val.drop(columns=['INS'])\n",
    "y_val = df_val['INS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable Boosting Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 12:50:19,294] A new study created in memory with name: no-name-bcce34c5-5b48-4250-906d-15d4118693be\n",
      "C:\\Users\\Sterling Hayden\\AppData\\Local\\Temp\\ipykernel_24744\\99524603.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the objective function for Bayesian optimization\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "        'max_bins': trial.suggest_categorical('max_bins', [64, 128, 256, 1024]),\n",
    "        'interactions': trial.suggest_categorical('interactions', [0.1, 0.3, 0.5, 0.7, 0.9]),\n",
    "        'outer_bags': trial.suggest_categorical('outer_bags', [5, 10, 20]),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_rounds': trial.suggest_categorical('max_rounds', [1000, 5000, 10000, 25000]),\n",
    "        'early_stopping_rounds': trial.suggest_categorical('early_stopping_rounds', [10, 50, 100]),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 3, 10),\n",
    "    }\n",
    "    \n",
    "    # Train the ExplainableBoostingClassifier\n",
    "    ebm = ExplainableBoostingClassifier(n_jobs=-1, random_state=88, **params)\n",
    "    ebm.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on the validation set (or use cross-validation)\n",
    "    y_pred_proba = ebm.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=88))\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "# Best hyperparameters and performance\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "\n",
    "# Train and evaluate the final model with best hyperparameters\n",
    "best_params = study.best_params\n",
    "final_model = ExplainableBoostingClassifier(n_jobs=-1, random_state=88, **best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "final_auc = roc_auc_score(y_val, final_model.predict_proba(X_val)[:, 1])\n",
    "print(\"Final Test AUC: {:.3f}\".format(final_auc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
